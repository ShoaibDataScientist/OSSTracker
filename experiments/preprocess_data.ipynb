{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the sentence data and extract conversational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd as xl\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import datetime, time\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "### Removing punctuation and common contractions during preprocessing of the text\n",
    "stop_words = list(punctuation) + [\"'s\",\"'m\",\"n't\",\"'re\",\"-\",\"'ll\",'...'] #+ stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to preprocess the data in the text:\n",
    "\n",
    "Preprocessing techniques:\n",
    "1. replace code snippets (encompassed by '\\`' or '\\`\\`\\`') with token 'CODE'\n",
    "2. replace URLs with token 'URL'\n",
    "3. Remove any sort of references (i.e. line beginning with '>' or corresponding to defined regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    ## Replace all code blocks with the token CODE\n",
    "    pattern = re.compile(r'```[^```]*?(```|$)', re.MULTILINE|re.DOTALL)\n",
    "    line = re.sub(pattern,'CODE',line)\n",
    "    pattern = re.compile(r'`[^`]*?`', re.MULTILINE|re.DOTALL)\n",
    "    line = re.sub(pattern,'CODE',line)\n",
    "    pattern = re.compile(r'(^|\\n)>.*')\n",
    "    line = re.sub(pattern,'\\nREFERENCE',line)\n",
    "    pattern = re.compile(r'On [\\d]+ \\w+ [\\d]+ at [\\d]+:[\\d]+, \\w+ \\w+ <.*?> wrote:')\n",
    "    line = re.sub(pattern,'REFERENCE',line)\n",
    "    pattern = re.compile(r'On \\w+, \\w+ \\d+, \\d+, \\d+:\\d+ (A|P)M \\w+ \\w+ .*? wrote:')\n",
    "    line = re.sub(pattern,'REFERENCE',line)\n",
    "    pattern = re.compile(r'([\\n\\t\\s]*?REFERENCE)+',re.MULTILINE|re.DOTALL)\n",
    "    line = re.sub(pattern,'\\nREFERENCE',line)\n",
    "    ## Replace URLs which are surrounded by brackets\n",
    "    pattern = re.compile(r'[\\[\\(]http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+[\\]\\)]')\n",
    "    line = re.sub(pattern,' URL ',line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from Annotated Data File:\n",
    "\n",
    "The file is an excel workbook with each sheet containing the sentences in the discussion thread of one issue. It was generated from the Atlas annotation tool.\n",
    "\n",
    "In order to capture conversational features, it was necessary to be able to link the comment sentences back to the information present in the github API response. To do so, time and author of the comment was annotated as METADATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 15\n",
      "[   '1 37_tensorflow.doc',\n",
      "    '2 1122_tensorflow.doc',\n",
      "    '3 197_spaCy.doc',\n",
      "    '4 285_spaCy.doc',\n",
      "    '5 429_spaCy.doc',\n",
      "    '6 1585_scikit-learn.doc',\n",
      "    '7 2889_scikit-learn.doc',\n",
      "    '8 10521_scikit-learn.doc',\n",
      "    '9 15_spaCy.doc',\n",
      "    '10 7951_tensorflow.doc',\n",
      "    '11 9393_scikit-learn.doc',\n",
      "    '12 8191_tensorflow.doc',\n",
      "    '13 125_spaCy.doc',\n",
      "    '14 6665_scikit-learn.doc',\n",
      "    '15 15604_tensorflow.doc']\n"
     ]
    }
   ],
   "source": [
    "xl_file = pd.ExcelFile(\"../data/annotated_data_with_metadata.xlsx\")\n",
    "print(\"Total number of documents: \"+str(len(xl_file.sheet_names)))\n",
    "pp.pprint(xl_file.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function helpers to get some of the conversational features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "eng = spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def str_to_datetime(date_string):\n",
    "    return datetime.datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "def time_between(date1, date2):\n",
    "    return (date2 - date1)\n",
    "\n",
    "word_count = lambda sentence: len([x for x in list(map(str,parser(sentence))) if x not in stop_words])\n",
    "\n",
    "def longest_sentence_length(sentence_list):\n",
    "    return max([word_count(sentence) for sentence in sentence_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess sentence content and extract conversational data:\n",
    "\n",
    "This function also reads from the file containing the comments obtained in json format from the github API for the issue. This is to get meta-information such as the created_at time of the comment, the author, etc. Matching of the annotated sentence to the json response from which it was etracted was done using the timestamp in METADATA content as mentioned above. Matching text was not recommended due to formatting issues as well as duplicate sentences. Matching authors was also not possible due to the occasional change in usernames of a single user. Hence, the created_at timestamp of the comment is used.\n",
    "\n",
    "The comment data is stored in a pandas dataframe with each row containing the Text Content and Conversational features.\n",
    "\n",
    "It stores the dictionary of format `{filename: pandas comments dataframe}` in the file *data_by_document.pkl*.\n",
    "\n",
    "It stores all the data in the file *all_data.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single-coded quotations per document: \n",
      "-----1 37_tensorflow.doc-----\n",
      "Different Name: 54 andrewjaykeller aj-ptw\n",
      "Different Name: 126 aakreidler Tim15\n",
      "Different Name: 216 madnai DecafManiac\n",
      "Total number of sentences: 436\n",
      "Time taken: 0:00:03.836733\n",
      "-----2 1122_tensorflow.doc-----\n",
      "Total number of sentences: 431\n",
      "Time taken: 0:06:37.870672\n",
      "-----3 197_spaCy.doc-----\n",
      "Total number of sentences: 136\n",
      "Time taken: 0:00:00.893212\n",
      "-----4 285_spaCy.doc-----\n",
      "Total number of sentences: 220\n",
      "Time taken: 0:00:01.335591\n",
      "-----5 429_spaCy.doc-----\n",
      "Total number of sentences: 86\n",
      "Time taken: 0:00:25.572686\n",
      "-----6 1585_scikit-learn.doc-----\n",
      "Total number of sentences: 271\n",
      "Time taken: 0:00:02.121975\n",
      "-----7 2889_scikit-learn.doc-----\n",
      "Total number of sentences: 329\n",
      "Time taken: 0:00:02.599940\n",
      "-----8 10521_scikit-learn.doc-----\n",
      "Total number of sentences: 250\n",
      "Time taken: 0:00:01.941520\n",
      "-----9 15_spaCy.doc-----\n",
      "Total number of sentences: 108\n",
      "Time taken: 0:00:00.906870\n",
      "-----10 7951_tensorflow.doc-----\n",
      "Different Name: 130 llvim biolee\n",
      "Total number of sentences: 646\n",
      "Time taken: 0:58:25.556722\n",
      "-----11 9393_scikit-learn.doc-----\n",
      "Total number of sentences: 233\n",
      "Time taken: 0:00:01.690657\n",
      "-----12 8191_tensorflow.doc-----\n",
      "Total number of sentences: 269\n",
      "Time taken: 0:00:01.971760\n",
      "-----13 125_spaCy.doc-----\n",
      "Total number of sentences: 279\n",
      "Time taken: 0:00:01.709639\n",
      "-----14 6665_scikit-learn.doc-----\n",
      "Total number of sentences: 318\n",
      "Time taken: 0:00:38.502730\n",
      "-----15 15604_tensorflow.doc-----\n",
      "Different Name: 79 iappp buysilver\n",
      "Different Name: 100 pavanchhatpar pavan-08\n",
      "Incorrect: 105 2018-07-04T01:56:00Z 2018-06-29 10:29:38\n",
      "Incorrect: 106 2018-07-16T03:05:59Z 2018-07-04 01:56:00\n",
      "Total number of sentences: 318\n",
      "Time taken: 0:00:02.242257\n",
      "\n",
      "Total size of singly-labelled data: 4330\n",
      "Total size of singly-labelled data: 4330\n",
      "Total Time taken: 1:06:28.825684\n"
     ]
    }
   ],
   "source": [
    "data_by_document = {}\n",
    "metadata_by_document = {}\n",
    "all_data = pd.DataFrame(columns=['Document','Text Content','Code','Full Length','len','tloc','cloc','tpos1','tpos2','clen','tlen','ppau','npau','aa','begauth','has_code','first_turn','last_turn'])\n",
    "total_length = 0\n",
    "\n",
    "print(\"Number of single-coded quotations per document: \")\n",
    "process_start_time = time.time()\n",
    "for sheet_name in xl_file.sheet_names:\n",
    "    sheet_start_time = time.time()\n",
    "    print('-----'+sheet_name+'-----')\n",
    "    issue_number, repo_name = sheet_name.split(' ')[1].split('_')\n",
    "    repo_name = repo_name.split('.')[0]\n",
    "    folder_name = repo_name+'_'+repo_name\n",
    "    if repo_name == 'spaCy':\n",
    "        folder_name = 'explosion_spaCy'\n",
    "    folder_path = os.path.join('../data/chosen_issues',folder_name)\n",
    "\n",
    "    ### Load the json file containing json api responses for each issue\n",
    "    with open(os.path.join(folder_path,'issue'+issue_number+'.txt')) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    quotation_data = xl_file.parse(sheet_name)\n",
    "    document_quotations_and_codes = pd.DataFrame(columns=['Document','Text Content','Code','Full Length','len','tloc','cloc','tpos1','tpos2','clen','tlen','ppau','npau','aa','begauth','has_code','first_turn','last_turn'])\n",
    "    metadata_document_quotations_and_codes = pd.DataFrame(columns=['Text Content','Code','Full Length','len','location'])\n",
    "\n",
    "    ### Extract and preprocess all sentences\n",
    "    for index, row in quotation_data.iterrows():\n",
    "        ### Currently ignoring multi-labelled sentences and labels with < 50 data points\n",
    "        if int(row['Number of Codes']) == 1 and row['Codes'] not in ['Future Plan','Content Management','Testing-Related']:\n",
    "            pp_text = preprocess(row['Text Content'])\n",
    "#             pp_text = row['Text Content']\n",
    "            location, orig_length = row['Location'].split(' ')\n",
    "            location = int(location.replace('location=',''))\n",
    "            orig_length = int(orig_length.replace('length=',''))\n",
    "            metadata_document_quotations_and_codes.loc[len(metadata_document_quotations_and_codes)] = [pp_text,row['Codes'],len(row['Text Content']),len(pp_text), location]\n",
    "    metadata_by_document[sheet_name] = metadata_document_quotations_and_codes\n",
    "\n",
    "    ### Sort all sentences by location in file because atlas.ti does not do this\n",
    "    metadata_document_quotations_and_codes.sort_values(by=['location'],inplace=True)\n",
    "\n",
    "    ### If no sentences found between two metadata tags - IGNORE the previous metadata\n",
    "    prev_code = ''\n",
    "    for index, row in metadata_document_quotations_and_codes.iterrows():\n",
    "        if prev_code == 'METADATA' and row['Code'] == 'METADATA':\n",
    "#             print(row['Text Content'])\n",
    "            metadata_document_quotations_and_codes.loc[index-1,'Code'] = 'N/A - METADATA'\n",
    "#             print(metadata_document_quotations_and_codes.loc[index-1])\n",
    "        prev_code = row['Code']\n",
    "\n",
    "    first_post_flag = 0\n",
    "    cloc = 0\n",
    "    tloc = 0\n",
    "    prev_tloc = 0\n",
    "    new_comment = 0\n",
    "\n",
    "    ### Calculate all conversational features\n",
    "    curr_post_time = ''\n",
    "    beg_comment = 0\n",
    "    comment_location = 0\n",
    "    for index, row in metadata_document_quotations_and_codes.iterrows():\n",
    "        if row['Code'] == 'METADATA':\n",
    "            beg_comment = 1\n",
    "            prev_comment_count = tloc\n",
    "            prev_comm_start = len(document_quotations_and_codes) - (prev_comment_count)\n",
    "            prev_comm_end = len(document_quotations_and_codes) - 1\n",
    "            if cloc!=0:\n",
    "                document_quotations_and_codes.loc[prev_comm_start:prev_comm_end,'tloc'] /= prev_comment_count\n",
    "                longest_sent_length_in_prev_comment = longest_sentence_length(document_quotations_and_codes.loc[prev_comm_start:prev_comm_end]['Text Content'].values)\n",
    "                document_quotations_and_codes.loc[prev_comm_start:prev_comm_end,'clen'] /= longest_sent_length_in_prev_comment\n",
    "            prev_tloc = tloc\n",
    "            tloc = 0\n",
    "            most_recent_metadata = row['Text Content']\n",
    "            prev_post_time = curr_post_time if curr_post_time else str_to_datetime(row['Text Content'].split(' ')[0])\n",
    "            curr_post_time, curr_poster = row['Text Content'].split(' ')\n",
    "            curr_post_time = str_to_datetime(curr_post_time)\n",
    "            if first_post_flag == 0:\n",
    "                first_post_time, first_poster = curr_post_time, curr_poster\n",
    "            first_post_flag += 1\n",
    "            json_comment = json_data[comment_location]\n",
    "            ## Sanity Check to match metadata from atlas.ti with github API response\n",
    "            if str_to_datetime(json_comment['created_at']) == curr_post_time:\n",
    "                if (json_comment['user']['login'] == curr_poster):\n",
    "                    #DO NOTHING\n",
    "#                   print('yay')\n",
    "                    yay = 1\n",
    "                else:\n",
    "                    print('Different Name: '+str(comment_location)+' '+json_comment['user']['login']+' '+str(curr_poster))\n",
    "            else:\n",
    "                print('Incorrect: '+str(comment_location)+' '+json_comment['created_at']+' '+str(curr_post_time))\n",
    "            comment_location+=1\n",
    "\n",
    "        elif row['Code'] == 'N/A - METADATA':\n",
    "            comment_location+=1\n",
    "        else:\n",
    "            if beg_comment == 1:\n",
    "                tloc = 1\n",
    "            else:\n",
    "                tloc += 1\n",
    "            has_code = True if 'CODE' in pp_text else False\n",
    "            first_turn = True if first_post_flag == 1 else False\n",
    "            cloc += 1\n",
    "            tpos1 = time_between(first_post_time,curr_post_time).total_seconds()/60\n",
    "            tpos2 = curr_post_time\n",
    "            ppau = time_between(prev_post_time, curr_post_time).total_seconds()/60\n",
    "            begauth = True if curr_poster == first_poster else False\n",
    "            aa = json_comment['author_association']\n",
    "            num_words = word_count(row['Text Content'])\n",
    "            clen = num_words\n",
    "            tlen = num_words\n",
    "            has_code = True if 'CODE' in row['Text Content'] else False\n",
    "            document_quotations_and_codes.loc[len(document_quotations_and_codes),['Text Content','Code','Full Length','len','tloc','cloc','tpos1','tpos2','clen','tlen','ppau','aa','begauth','has_code','first_turn','last_turn']] = [row['Text Content'], row['Code'], row['Full Length'], row['len'],tloc,cloc,tpos1,tpos2,clen,tlen, ppau, aa,begauth,has_code,first_turn,False]\n",
    "            if cloc != 1 and beg_comment == 1: document_quotations_and_codes.loc[prev_comm_start:prev_comm_end,'npau'] = ppau\n",
    "            if beg_comment == 1: beg_comment = 0\n",
    "\n",
    "    document_quotations_and_codes.loc[prev_comm_end+1:len(document_quotations_and_codes)+1,'last_turn'] = True\n",
    "    document_quotations_and_codes.loc[prev_comm_end+1:len(document_quotations_and_codes)+1,'npau'] = time_between(curr_post_time, curr_post_time).total_seconds()/60\n",
    "    last_post_time = curr_post_time\n",
    "    total_time = time_between(first_post_time, last_post_time).total_seconds()/60\n",
    "    num_sentences = len(document_quotations_and_codes)\n",
    "    longest_sent_length = longest_sentence_length(document_quotations_and_codes['Text Content'].values)\n",
    "    document_quotations_and_codes.tlen/=longest_sent_length\n",
    "    document_quotations_and_codes.tpos2=time_between(document_quotations_and_codes.tpos2,last_post_time).dt.total_seconds()/60\n",
    "    document_quotations_and_codes.tpos1/=total_time\n",
    "    document_quotations_and_codes.tpos2/=total_time\n",
    "    document_quotations_and_codes.cloc/=num_sentences\n",
    "    document_quotations_and_codes.ppau/=document_quotations_and_codes.ppau.max()\n",
    "    document_quotations_and_codes.npau/=document_quotations_and_codes.npau.max()\n",
    "    document_quotations_and_codes.Document = sheet_name\n",
    "    data_by_document[sheet_name] = document_quotations_and_codes\n",
    "\n",
    "    print(\"Total number of sentences: \"+str(num_sentences))\n",
    "    print(\"Time taken: \"+str(datetime.timedelta(seconds=(time.time()-sheet_start_time))))\n",
    "\n",
    "    total_length += len(document_quotations_and_codes)\n",
    "    all_data = all_data.append(document_quotations_and_codes)\n",
    "\n",
    "# print(all_data.iloc[0:3])\n",
    "print('\\nTotal size of singly-labelled data: '+str(total_length))\n",
    "print('Total size of singly-labelled data: '+str(len(all_data)))\n",
    "print(\"Total Time taken: \"+str(datetime.timedelta(seconds=(time.time()-process_start_time))))\n",
    "\n",
    "# Save preprocessed text in pickle file\n",
    "with open('../data/data_by_document.pkl', 'wb') as handle:\n",
    "    pickle.dump(data_by_document, handle)\n",
    "\n",
    "all_data.to_pickle('../data/all_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Document                                       Text Content  \\\n",
      "0  5 429_spaCy.doc               pipe(): ValueError Error parsing doc   \n",
      "1  5 429_spaCy.doc  I found strange behaviour using the CODE metho...   \n",
      "2  5 429_spaCy.doc  If you parse a document using CODE you can get...   \n",
      "\n",
      "                     Code Full Length  len      tloc       cloc tpos1  tpos2  \\\n",
      "0  Observed Bug Behaviour          36   36  0.111111  0.0116279     0    1.0   \n",
      "1  Observed Bug Behaviour          86   82  0.222222  0.0232558     0    1.0   \n",
      "2  Observed Bug Behaviour         111  100  0.333333  0.0348837     0    1.0   \n",
      "\n",
      "   clen       tlen ppau        npau    aa begauth has_code first_turn  \\\n",
      "0  0.25  0.0526316    0  0.00859827  NONE    True    False       True   \n",
      "1  0.65   0.136842    0  0.00859827  NONE    True     True       True   \n",
      "2     1   0.210526    0  0.00859827  NONE    True     True       True   \n",
      "\n",
      "  last_turn  \n",
      "0     False  \n",
      "1     False  \n",
      "2     False  \n",
      "-----------------------------------------------\n",
      "['pipe(): ValueError Error parsing doc'\n",
      " 'I found strange behaviour using the CODE method (only verified on german variant):'\n",
      " 'If you parse a document using CODE you can get a ValueError, while if i use CODE everything is fine.'\n",
      " \"I boiled it down to single words, while german words work, english words like 'windows' don't work.\"\n",
      " 'Steps to reproduce: CODE' 'Trace CODE']\n"
     ]
    }
   ],
   "source": [
    "print(data_by_document['5 429_spaCy.doc'].iloc[0:3])\n",
    "print(\"-----------------------------------------------\")\n",
    "print(data_by_document['5 429_spaCy.doc'].iloc[0:6]['Text Content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rethinking the CategoricalEncoder API ?'\n",
      " 'Based on some discussions we are having here and issues that are opened, we are having some doubts that CODE  URL  was the good choice of name (and since it is not released yet we have some room for change).'\n",
      " 'So summary of how it is now:'\n",
      " '-         The class name CODE says what type of data it accepts (categorical data)'\n",
      " '-         The keyword argument CODE specifies *how* to encode those data'\n",
      " 'But what to do in the following cases:']\n"
     ]
    }
   ],
   "source": [
    "print(data_by_document['8 10521_scikit-learn.doc'].iloc[0:6]['Text Content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Enhancement] Redesigning TensorFlow's input pipelines\"\n",
      " \"[**TL;DR:** We're designing a new input pipeline API for TensorFlow, and we'd like to collect your feature requests on this issue.]\"\n",
      " \"We've noticed that one of the biggest challenges in getting started with TensorFlow is how to load your own data into your programs.\"\n",
      " 'While TensorFlow has several methods that can be used to build complex input pipelines (such as [CODE] URL , [CODE] URL , etc.), they were designed for a particular use case (processing a static set of files repeatedly), and the average user experience with these methods is not great.'\n",
      " 'For example:'\n",
      " '*         Once you reach the end of a pipeline, it becomes closed and you can never use it again in the same session.']\n"
     ]
    }
   ],
   "source": [
    "print(data_by_document['10 7951_tensorflow.doc'].iloc[0:6]['Text Content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Debian test failures (was test_preserve_trustworthiness_approximately fails on 32bit: AssertionError: 0.89166666666666661 not greater than 0.9)'\n",
      " 'building 0.19b2 on debian/ubuntus ...'\n",
      " 'still ongoing but I see consistent failure on Debian stretch (nd90, current stable) and testing (nd100), 32bit only (ok on amd64 build):CODE'\n",
      " 'in both cases python-numpy is CODE (i.e. 1.12.1 numpy) and passed ok with numpy 1.8.2 in Debian jessie.'\n",
      " 'ping @ogrisel?'\n",
      " \"Interesting, it's a only on a combo of numpy 1.12.1 and 32 bit python...\"]\n"
     ]
    }
   ],
   "source": [
    "print(data_by_document['11 9393_scikit-learn.doc'].iloc[0:6]['Text Content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
